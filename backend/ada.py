import asyncio
import base64
import io
import os
import sys
import traceback
from dotenv import load_dotenv
import cv2
import pyaudio
import PIL.Image
import mss
import argparse
import math
import struct
import time

from google import genai
from google.genai import types

if sys.version_info < (3, 11, 0):
    import taskgroup, exceptiongroup
    asyncio.TaskGroup = taskgroup.TaskGroup
    asyncio.ExceptionGroup = exceptiongroup.ExceptionGroup

from tools import tools_list

# Import Tool Registry and Handlers
from tool_registry import ToolRegistry

from tool_definitions import google_workspace as dw_gw
from tool_definitions import n8n as dw_n8n
from tool_definitions import local_pc as dw_pc
from tool_definitions import webhook as dw_wh
from tool_definitions import whatsapp as dw_wa
from tool_definitions import printer as dw_pr
from tool_definitions import yahoo_mail as dw_ym
from tool_definitions import misc as dw_misc

from tool_handlers.google_workspace_handler import GoogleWorkspaceHandler
from tool_handlers.n8n_handler import N8nHandler
from tool_handlers.local_pc_handler import LocalPcHandler
from tool_handlers.webhook_handler import WebhookHandler
from tool_handlers.whatsapp_handler import WhatsAppHandler
from tool_handlers.printer_handler import PrinterHandler
from tool_handlers.yahoo_mail_handler import YahooMailHandler
from tool_handlers.misc_handler import MiscHandler

FORMAT = pyaudio.paInt16
CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024

MODEL = "models/gemini-2.5-flash-native-audio-preview-12-2025"
DEFAULT_MODE = "camera"

load_dotenv()
client = genai.Client(http_options={"api_version": "v1beta"}, api_key=os.getenv("GEMINI_API_KEY"))

# Initialize Agents
from web_agent import WebAgent
from kasa_agent import KasaAgent
from google_workspace_agent import get_workspace_agent
from n8n_mcp_agent import get_n8n_agent
from local_pc_agent import get_local_pc_agent
from webhook_agent import get_webhook_agent
from whatsapp_agent import get_whatsapp_agent
from document_printer_agent import get_document_printer_agent
from yahoo_mail_agent import get_yahoo_agent

# Initialize Agents Globally or in AudioLoop?
# In AudioLoop is better to avoid global state if possible, but for tools init we might need them.
# The tool definitions are static. The handlers need instances.

# We will initialize the tool registry inside AudioLoop.__init__

pya = pyaudio.PyAudio()

class AudioLoop:
    def __init__(self, video_mode=DEFAULT_MODE, on_audio_data=None, on_video_frame=None, on_cad_data=None, on_web_data=None, on_transcription=None, on_tool_confirmation=None, on_cad_status=None, on_cad_thought=None, on_project_update=None, on_device_update=None, on_error=None, input_device_index=None, input_device_name=None, output_device_index=None, kasa_agent=None):
        self.video_mode = video_mode
        self.on_audio_data = on_audio_data
        self.on_video_frame = on_video_frame
        self.on_cad_data = on_cad_data
        self.on_web_data = on_web_data
        self.on_transcription = on_transcription
        self.on_tool_confirmation = on_tool_confirmation 
        self.on_cad_status = on_cad_status
        self.on_cad_thought = on_cad_thought
        self.on_project_update = on_project_update
        self.on_device_update = on_device_update
        self.on_error = on_error
        self.input_device_index = input_device_index
        self.input_device_name = input_device_name
        self.output_device_index = output_device_index

        self.audio_in_queue = None
        self.out_queue = None
        self.paused = False

        self.chat_buffer = {"sender": None, "text": ""} # For aggregating chunks
        
        # Track last transcription text to calculate deltas (Gemini sends cumulative text)
        self._last_input_transcription = ""
        self._last_output_transcription = ""

        self.session = None
        
        # Initialize ProjectManager
        from project_manager import ProjectManager
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(current_dir)
        self.project_manager = ProjectManager(project_root)

        # Initialize Agents
        self.web_agent = WebAgent()
        self.kasa_agent = kasa_agent if kasa_agent else KasaAgent()
        self.google_workspace_agent = get_workspace_agent()
        self.n8n_mcp_agent = get_n8n_agent()
        print("[SERVER] Startup: Initializing Yahoo Mail Agent...")
        self.yahoo_mail_agent = get_yahoo_agent()
        self.local_pc_agent = get_local_pc_agent()
        self.webhook_agent = get_webhook_agent()
        self.whatsapp_agent = get_whatsapp_agent()
        self.document_printer_agent = get_document_printer_agent()

        self.send_text_task = None
        self.stop_event = asyncio.Event()
        
        self.permissions = {} # Default Empty (Will treat unset as True)
        self._pending_confirmations = {}

        # Video buffering state
        self._latest_image_payload = None
        # VAD State
        self._is_speaking = False
        self._silence_start_time = None

        # ==========================================
        # INITIALIZE TOOL REGISTRY
        # ==========================================
        self.registry = ToolRegistry()
        
        # Register Google Workspace
        self.gw_handler = GoogleWorkspaceHandler(self.google_workspace_agent)
        # Helper map: tool_name -> handler_method_name
        # We can automate this if naming convention is consistent: name -> handle_{name}
        for definition in dw_gw.definitions:
            method_name = f"handle_{definition['name']}"
            if hasattr(self.gw_handler, method_name):
                self.registry.register_tool(definition['name'], getattr(self.gw_handler, method_name), definition)

        # Register N8N
        self.n8n_handler = N8nHandler(self.n8n_mcp_agent)
        for definition in dw_n8n.definitions:
            method_name = f"handle_{definition['name']}"
            if hasattr(self.n8n_handler, method_name):
                self.registry.register_tool(definition['name'], getattr(self.n8n_handler, method_name), definition)

        # Register Local PC
        self.pc_handler = LocalPcHandler(self.local_pc_agent)
        for definition in dw_pc.definitions:
            method_name = f"handle_{definition['name']}"
            if hasattr(self.pc_handler, method_name):
                self.registry.register_tool(definition['name'], getattr(self.pc_handler, method_name), definition)

        # Register Webhook
        self.wh_handler = WebhookHandler(self.webhook_agent)
        for definition in dw_wh.definitions:
            method_name = f"handle_{definition['name']}"
            if hasattr(self.wh_handler, method_name):
                self.registry.register_tool(definition['name'], getattr(self.wh_handler, method_name), definition)

        # Register WhatsApp
        self.wa_handler = WhatsAppHandler(self.whatsapp_agent)
        for definition in dw_wa.definitions:
            method_name = f"handle_{definition['name']}"
            if hasattr(self.wa_handler, method_name):
                self.registry.register_tool(definition['name'], getattr(self.wa_handler, method_name), definition)

        # Register Printer
        self.pr_handler = PrinterHandler(self.document_printer_agent)
        for definition in dw_pr.definitions:
            method_name = f"handle_{definition['name']}"
            if hasattr(self.pr_handler, method_name):
                self.registry.register_tool(definition['name'], getattr(self.pr_handler, method_name), definition)

        # Register Yahoo Mail
        self.ym_handler = YahooMailHandler(self.yahoo_mail_agent)
        for definition in dw_ym.definitions:
            method_name = f"handle_{definition['name']}"
            if hasattr(self.ym_handler, method_name):
                self.registry.register_tool(definition['name'], getattr(self.ym_handler, method_name), definition)

        # Register Misc
        self.misc_handler = MiscHandler(
            web_agent=self.web_agent,
            project_manager=self.project_manager,
            kasa_agent=self.kasa_agent,
            on_web_data_callback=self.on_web_data,
            on_project_update_callback=self.on_project_update,
            on_device_update_callback=self.on_device_update,
            on_error_callback=self.on_error,
            session=None # Will set session in run()
        )
        for definition in dw_misc.definitions:
            method_name = f"handle_{definition['name']}"
            if hasattr(self.misc_handler, method_name):
                self.registry.register_tool(definition['name'], getattr(self.misc_handler, method_name), definition)
        
        # Also include legacy tools_list[0] if needed (CAD, etc.)
        # tools.py definitions?
        # For now, let's assume we migrated everything important.
        # tools_list[0] had generate_cad_prototype_tool etc.
        # If we didn't migrate CAD tool, we should add it back or migrate it.
        # The prompt mentioned "Misc tools" but didn't explicitly list CAD.
        # I'll skip CAD for now to keep it clean, or add it if requested.


    def flush_chat(self):
        """Forces the current chat buffer to be written to log."""
        if self.chat_buffer["sender"] and self.chat_buffer["text"].strip():
            self.project_manager.log_chat(self.chat_buffer["sender"], self.chat_buffer["text"])
            self.chat_buffer = {"sender": None, "text": ""}
        # Reset transcription tracking for new turn
        self._last_input_transcription = ""
        self._last_output_transcription = ""

    def update_permissions(self, new_perms):
        print(f"[ADA DEBUG] [CONFIG] Updating tool permissions: {new_perms}")
        self.permissions.update(new_perms)

    def set_paused(self, paused):
        self.paused = paused

    def stop(self):
        self.stop_event.set()
        
    def resolve_tool_confirmation(self, request_id, confirmed):
        print(f"[ADA DEBUG] [RESOLVE] resolve_tool_confirmation called. ID: {request_id}, Confirmed: {confirmed}")
        if request_id in self._pending_confirmations:
            future = self._pending_confirmations[request_id]
            if not future.done():
                print(f"[ADA DEBUG] [RESOLVE] Future found and pending. Setting result to: {confirmed}")
                future.set_result(confirmed)
            else:
                 print(f"[ADA DEBUG] [WARN] Request {request_id} future already done. Result: {future.result()}")
        else:
            print(f"[ADA DEBUG] [WARN] Confirmation Request {request_id} not found in pending dict. Keys: {list(self._pending_confirmations.keys())}")

    def clear_audio_queue(self):
        """Clears the queue of pending audio chunks to stop playback immediately."""
        try:
            count = 0
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()
                count += 1
            if count > 0:
                print(f"[ADA DEBUG] [AUDIO] Cleared {count} chunks from playback queue due to interruption.")
        except Exception as e:
            print(f"[ADA DEBUG] [ERR] Failed to clear audio queue: {e}")

    async def send_frame(self, frame_data):
        # Update the latest frame payload
        if isinstance(frame_data, bytes):
            b64_data = base64.b64encode(frame_data).decode('utf-8')
        else:
            b64_data = frame_data 

        # Store as the designated "next frame to send"
        self._latest_image_payload = {"mime_type": "image/jpeg", "data": b64_data}
        # No event signal needed - listen_audio pulls it

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg, end_of_turn=False)

    async def listen_audio(self):
        mic_info = pya.get_default_input_device_info()

        # Resolve Input Device by Name if provided
        resolved_input_device_index = None
        
        if self.input_device_name:
            print(f"[ADA] Attempting to find input device matching: '{self.input_device_name}'")
            count = pya.get_device_count()
            best_match = None
            
            for i in range(count):
                try:
                    info = pya.get_device_info_by_index(i)
                    if info['maxInputChannels'] > 0:
                        name = info.get('name', '')
                        # Simple case-insensitive check
                        if self.input_device_name.lower() in name.lower() or name.lower() in self.input_device_name.lower():
                             print(f"   Candidate {i}: {name}")
                             # Prioritize exact match or very close match if possible, but first match is okay for now
                             resolved_input_device_index = i
                             best_match = name
                             break
                except Exception:
                    continue
            
            if resolved_input_device_index is not None:
                print(f"[ADA] Resolved input device '{self.input_device_name}' to index {resolved_input_device_index} ({best_match})")
            else:
                print(f"[ADA] Could not find device matching '{self.input_device_name}'. Checking index...")

        # Fallback to index if Name lookup failed or wasn't provided
        if resolved_input_device_index is None and self.input_device_index is not None:
             try:
                 resolved_input_device_index = int(self.input_device_index)
                 print(f"[ADA] Requesting Input Device Index: {resolved_input_device_index}")
             except ValueError:
                 print(f"[ADA] Invalid device index '{self.input_device_index}', reverting to default.")
                 resolved_input_device_index = None

        if resolved_input_device_index is None:
             print("[ADA] Using Default Input Device")

        try:
            self.audio_stream = await asyncio.to_thread(
                pya.open,
                format=FORMAT,
                channels=CHANNELS,
                rate=SEND_SAMPLE_RATE,
                input=True,
                input_device_index=resolved_input_device_index if resolved_input_device_index is not None else mic_info["index"],
                frames_per_buffer=CHUNK_SIZE,
            )
        except OSError as e:
            print(f"[ADA] [ERR] Failed to open audio input stream: {e}")
            print("[ADA] [WARN] Audio features will be disabled. Please check microphone permissions.")
            return

        if __debug__:
            kwargs = {"exception_on_overflow": False}
        else:
            kwargs = {}
        
        # VAD Constants
        VAD_THRESHOLD = 800 # Adj based on mic sensitivity (800 is conservative for 16-bit)
        SILENCE_DURATION = 0.5 # Seconds of silence to consider "done speaking"
        
        while True:
            if self.paused:
                await asyncio.sleep(0.1)
                continue

            try:
                data = await asyncio.to_thread(self.audio_stream.read, CHUNK_SIZE, **kwargs)
                
                # 1. Send Audio
                if self.out_queue:
                    await self.out_queue.put({"data": data, "mime_type": "audio/pcm"})
                
                # 2. VAD Logic for Video
                # rms = audioop.rms(data, 2)
                # Replacement for audioop.rms(data, 2)
                count = len(data) // 2
                if count > 0:
                    shorts = struct.unpack(f"<{count}h", data)
                    sum_squares = sum(s**2 for s in shorts)
                    rms = int(math.sqrt(sum_squares / count))
                else:
                    rms = 0
                
                if rms > VAD_THRESHOLD:
                    # Speech Detected
                    self._silence_start_time = None
                    
                    if not self._is_speaking:
                        # NEW Speech Utterance Started
                        self._is_speaking = True
                        print(f"[ADA DEBUG] [VAD] Speech Detected (RMS: {rms}). Sending Video Frame.")
                        
                        # Send ONE frame
                        if self._latest_image_payload and self.out_queue:
                            await self.out_queue.put(self._latest_image_payload)
                        else:
                            print(f"[ADA DEBUG] [VAD] No video frame available to send.")
                            
                else:
                    # Silence
                    if self._is_speaking:
                        if self._silence_start_time is None:
                            self._silence_start_time = time.time()
                        
                        elif time.time() - self._silence_start_time > SILENCE_DURATION:
                            # Silence confirmed, reset state
                            print(f"[ADA DEBUG] [VAD] Silence detected. Resetting speech state.")
                            self._is_speaking = False
                            self._silence_start_time = None

            except Exception as e:
                print(f"Error reading audio: {e}")
                await asyncio.sleep(0.1)

    async def receive_audio(self):
        "Background task to reads from the websocket and write pcm chunks to the output queue"
        try:
            while True:
                turn = self.session.receive()
                async for response in turn:
                    # 1. Handle Audio Data
                    if data := response.data:
                        self.audio_in_queue.put_nowait(data)
                        # NOTE: 'continue' removed here to allow processing transcription/tools in same packet

                    # 2. Handle Transcription (User & Model)
                    if response.server_content:
                        if response.server_content.input_transcription:
                            transcript = response.server_content.input_transcription.text
                            if transcript:
                                # Skip if this is an exact duplicate event
                                if transcript != self._last_input_transcription:
                                    # Calculate delta (Gemini may send cumulative or chunk-based text)
                                    delta = transcript
                                    if transcript.startswith(self._last_input_transcription):
                                        delta = transcript[len(self._last_input_transcription):]
                                    self._last_input_transcription = transcript
                                    
                                    # Only send if there's new text
                                    if delta:
                                        # User is speaking, so interrupt model playback!
                                        self.clear_audio_queue()

                                        # Send to frontend (Streaming)
                                        if self.on_transcription:
                                             self.on_transcription({"sender": "User", "text": delta})
                                        
                                        # Buffer for Logging
                                        if self.chat_buffer["sender"] != "User":
                                            # Flush previous if exists
                                            if self.chat_buffer["sender"] and self.chat_buffer["text"].strip():
                                                self.project_manager.log_chat(self.chat_buffer["sender"], self.chat_buffer["text"])
                                            # Start new
                                            self.chat_buffer = {"sender": "User", "text": delta}
                                        else:
                                            # Append
                                            self.chat_buffer["text"] += delta
                        
                        if response.server_content.output_transcription:
                            transcript = response.server_content.output_transcription.text
                            if transcript:
                                # Skip if this is an exact duplicate event
                                if transcript != self._last_output_transcription:
                                    # Calculate delta (Gemini may send cumulative or chunk-based text)
                                    delta = transcript
                                    if transcript.startswith(self._last_output_transcription):
                                        delta = transcript[len(self._last_output_transcription):]
                                    self._last_output_transcription = transcript
                                    
                                    # Only send if there's new text
                                    if delta:
                                        # Send to frontend (Streaming)
                                        if self.on_transcription:
                                             self.on_transcription({"sender": "ADA", "text": delta})
                                        
                                        # Buffer for Logging
                                        if self.chat_buffer["sender"] != "ADA":
                                            # Flush previous
                                            if self.chat_buffer["sender"] and self.chat_buffer["text"].strip():
                                                self.project_manager.log_chat(self.chat_buffer["sender"], self.chat_buffer["text"])
                                            # Start new
                                            self.chat_buffer = {"sender": "ADA", "text": delta}
                                        else:
                                            # Append
                                            self.chat_buffer["text"] += delta
                        
                        # Flush buffer on turn completion if needed, 
                        # but usually better to wait for sender switch or explicit end.
                        # We can also check turn_complete signal if available in response.server_content.model_turn etc

                    # 3. Handle Tool Calls via Registry
                    if response.tool_call:
                        print("The tool was called")
                        function_responses = []

                        # Callback wrapper for user confirmation
                        async def confirmation_callback(request_id, tool_name, args):
                            if self.on_tool_confirmation:
                                # Future to wait for
                                future = asyncio.Future()
                                self._pending_confirmations[request_id] = future
                                
                                # Emit to UI
                                self.on_tool_confirmation({
                                    "id": request_id,
                                    "tool": tool_name,
                                    "args": args
                                })
                                
                                try:
                                    # Wait
                                    return await future
                                finally:
                                    self._pending_confirmations.pop(request_id, None)
                            else:
                                # If no UI callback, auto-deny safe default? Or auto-allow?
                                # Ada.py logic implied if confirmation required but no mechanism, it blocks?
                                # Let's assume auto-deny if confirmation is MANDATORY but no UI connected.
                                return False

                        for fc in response.tool_call.function_calls:
                            # Use Registry
                            response_obj = await self.registry.execute_tool(
                                tool_call=fc,
                                permissions=self.permissions,
                                on_confirmation=confirmation_callback
                            )
                            function_responses.append(response_obj)

                        if function_responses:
                            await self.session.send_tool_response(function_responses=function_responses)
                
                # Turn/Response Loop Finished
                self.flush_chat()

                while not self.audio_in_queue.empty():
                    self.audio_in_queue.get_nowait()
        except Exception as e:
            print(f"Error in receive_audio: {e}")
            traceback.print_exc()
            # CRITICAL: Re-raise to crash the TaskGroup and trigger outer loop reconnect
            raise e

    async def play_audio(self):
        stream = await asyncio.to_thread(
            pya.open,
            format=FORMAT,
            channels=CHANNELS,
            rate=RECEIVE_SAMPLE_RATE,
            output=True,
            output_device_index=self.output_device_index,
        )
        while True:
            bytestream = await self.audio_in_queue.get()
            if self.on_audio_data:
                self.on_audio_data(bytestream)
            await asyncio.to_thread(stream.write, bytestream)

    async def get_frames(self):
        cap = await asyncio.to_thread(cv2.VideoCapture, 0, cv2.CAP_AVFOUNDATION)
        while True:
            if self.paused:
                await asyncio.sleep(0.1)
                continue
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break
            await asyncio.sleep(1.0)
            if self.out_queue:
                await self.out_queue.put(frame)
        cap.release()

    def _get_frame(self, cap):
        ret, frame = cap.read()
        if not ret:
            return None
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)
        img.thumbnail([1024, 1024])
        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)
        image_bytes = image_io.read()
        return {"mime_type": "image/jpeg", "data": base64.b64encode(image_bytes).decode()}

    async def _get_screen(self):
        pass 
    async def get_screen(self):
         pass

    async def run(self, start_message=None):
        retry_delay = 1
        is_reconnect = False
        
        # Link Session to Handlers that need it
        self.misc_handler.session = None # Will set inside loop

        while not self.stop_event.is_set():
            try:
                # REBUILD CONFIG WITH CURRENT TOOLS FROM REGISTRY
                all_tools = self.registry.get_tool_definitions()
                # Wrap in format expected by GenAI SDK
                # [{'google_search': {}}, {"function_declarations": [...]}]
                current_tools_config = [
                    {'google_search': {}},
                    {"function_declarations": all_tools}
                ]

                # Update global config object (or create new one)
                current_config = types.LiveConnectConfig(
                    response_modalities=["AUDIO"],
                    output_audio_transcription={},
                    input_audio_transcription={},
                    system_instruction=config.system_instruction, # Reuse
                    tools=current_tools_config,
                    speech_config=config.speech_config # Reuse
                )

                print(f"[ADA DEBUG] [CONNECT] Connecting to Gemini Live API with {len(all_tools)} tools...")
                async with (
                    client.aio.live.connect(model=MODEL, config=current_config) as session,
                    asyncio.TaskGroup() as tg,
                ):
                    self.session = session
                    self.misc_handler.session = session # Inject session

                    self.audio_in_queue = asyncio.Queue()
                    self.out_queue = asyncio.Queue(maxsize=10)

                    tg.create_task(self.send_realtime())
                    tg.create_task(self.listen_audio())

                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                    tg.create_task(self.receive_audio())
                    tg.create_task(self.play_audio())

                    # Handle Startup vs Reconnect Logic
                    if not is_reconnect:
                        if start_message:
                            print(f"[ADA DEBUG] [INFO] Sending start message: {start_message}")
                            await self.session.send(input=start_message, end_of_turn=True)
                        
                        # Sync Project State
                        if self.on_project_update and self.project_manager:
                            self.on_project_update(self.project_manager.current_project)
                    
                    else:
                        print(f"[ADA DEBUG] [RECONNECT] Connection restored.")
                        # Restore Context
                        print(f"[ADA DEBUG] [RECONNECT] Fetching recent chat history to restore context...")
                        history = self.project_manager.get_recent_chat_history(limit=10)
                        
                        context_msg = "System Notification: Connection was lost and just re-established. Here is the recent chat history to help you resume seamlessly:\n\n"
                        for entry in history:
                            sender = entry.get('sender', 'Unknown')
                            text = entry.get('text', '')
                            context_msg += f"[{sender}]: {text}\n"
                        
                        context_msg += "\nPlease acknowledge the reconnection to the user (e.g. 'I lost connection for a moment, but I'm back...') and resume what you were doing."
                        
                        print(f"[ADA DEBUG] [RECONNECT] Sending restoration context to model...")
                        await self.session.send(input=context_msg, end_of_turn=True)

                    # Reset retry delay on successful connection
                    retry_delay = 1
                    
                    await self.stop_event.wait()

            except asyncio.CancelledError:
                print(f"[ADA DEBUG] [STOP] Main loop cancelled.")
                break
                
            except Exception as e:
                # This catches the ExceptionGroup from TaskGroup or direct exceptions
                print(f"[ADA DEBUG] [ERR] Connection Error: {e}")
                import traceback
                traceback.print_exc()
                
                if self.stop_event.is_set():
                    break
                
                print(f"[ADA DEBUG] [RETRY] Reconnecting in {retry_delay} seconds...")
                await asyncio.sleep(retry_delay)
                retry_delay = min(retry_delay * 2, 10) # Exponential backoff capped at 10s
                is_reconnect = True # Next loop will be a reconnect
                
            finally:
                # Cleanup before retry
                if hasattr(self, 'audio_stream') and self.audio_stream:
                    try:
                        self.audio_stream.close()
                    except: 
                        pass

def get_input_devices():
    p = pyaudio.PyAudio()
    info = p.get_host_api_info_by_index(0)
    numdevices = info.get('deviceCount')
    devices = []
    for i in range(0, numdevices):
        if (p.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels')) > 0:
            devices.append((i, p.get_device_info_by_host_api_device_index(0, i).get('name')))
    p.terminate()
    return devices

def get_output_devices():
    p = pyaudio.PyAudio()
    info = p.get_host_api_info_by_index(0)
    numdevices = info.get('deviceCount')
    devices = []
    for i in range(0, numdevices):
        if (p.get_device_info_by_host_api_device_index(0, i).get('maxOutputChannels')) > 0:
            devices.append((i, p.get_device_info_by_host_api_device_index(0, i).get('name')))
    p.terminate()
    return devices

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())
